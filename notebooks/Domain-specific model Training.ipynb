{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5858000,"sourceType":"datasetVersion","datasetId":3368972},{"sourceId":6042246,"sourceType":"datasetVersion","datasetId":3456002},{"sourceId":7365741,"sourceType":"datasetVersion","datasetId":3438212},{"sourceId":7813207,"sourceType":"datasetVersion","datasetId":4564823},{"sourceId":8405221,"sourceType":"datasetVersion","datasetId":3452365}],"dockerImageVersionId":30512,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install vncorenlp==1.0.3 --quiet\n!sudo apt-get install git-lfs\n!pip install sentencepiece --quiet\n!pip install tokenizer --quiet\n!pip install underthesea --quiet","metadata":{"id":"biqvUkDE-VVp","outputId":"36d468de-4b8b-461e-fbfd-8c7a4145b945","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom underthesea import text_normalize\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport requests\nimport random\n\nfrom torch.utils.data import TensorDataset","metadata":{"id":"5ogB1qGraiqU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n#     torch.use_deterministic_algorithms(True)\n\nfix_seed(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"CHjofGVV4lOp"}},{"cell_type":"markdown","source":"## UiT-VSFC","metadata":{}},{"cell_type":"code","source":"uit_train_data = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uit/uit_train_data.csv')\nuit_val_data = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uit/uit_val_data.csv')\nuit_test_data = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uit/uit_test_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UET Data","metadata":{}},{"cell_type":"code","source":"train_data_1 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uetcfs/uetcfs_train_data.csv')\nval_data_1 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uetcfs/uetcfs_val_data.csv')\ntest_data_1 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/uetcfs/uetcfs_test_data.csv')\n\ntrain_data_2 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/sguet/sguet_train_data.csv')\nval_data_2 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/sguet/sguet_val_data.csv')\ntest_data_2 = pd.read_csv('/kaggle/input/merge-fixed-dataset-712/dataset/sguet/sguet_test_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"!git-lfs clone https://github.com/vncorenlp/VnCoreNLP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\n\n# paste path to VnCoreNLP-1.1.1.jar\nrdrsegmenter = VnCoreNLP(\"/kaggle/working/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_segment(text):\n    return \"\".join([\" \".join(sen) for sen in rdrsegmenter.tokenize(text_normalize(text))])","metadata":{"id":"2QCDTnABT0_O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = re.sub(r'<[^>]*>', '', text)\n    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n    text = re.sub(r'\\w*\\d\\w*', ' ', text).strip()\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    text = text.lower()\n    text = word_segment(text)\n    return text","metadata":{"id":"1KZlUbNySa0v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uit_train_data['content'] = uit_train_data.content.progress_apply(partial(preprocess_text),)\nuit_val_data['content'] = uit_val_data.content.progress_apply(partial(preprocess_text),)\nuit_test_data['content'] = uit_test_data.content.progress_apply(partial(preprocess_text),)","metadata":{"id":"jsnOaLedSs7u","outputId":"cf652b0e-8a2c-4d67-b746-ae0684ece8f6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_2['content'] = train_data_2.content.progress_apply(partial(preprocess_text),)\nval_data_2['content'] = val_data_2.content.progress_apply(partial(preprocess_text),)\ntest_data_2['content'] = test_data_2.content.progress_apply(partial(preprocess_text),)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_1['content'] = train_data_1.content.progress_apply(partial(preprocess_text),)\nval_data_1['content'] = val_data_1.content.progress_apply(partial(preprocess_text),)\ntest_data_1['content'] = test_data_1.content.progress_apply(partial(preprocess_text),)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")","metadata":{"id":"v2VHBhK2VZkv","outputId":"7acde7a1-8281-4fc0-feec-4226dba1261e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nMAX_LEN = 256\nBATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 2e-5","metadata":{"id":"oRTkP2fDWLtf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"DY914VEEWOon","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encoding(data, tokenizer, max_token_len=128):\n    contents = data['content']\n    input_ids = []\n    attention_masks = []\n\n    for index, content in enumerate(contents):\n        encoded = tokenizer.encode_plus(\n            content,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=max_token_len,\n            padding=\"max_length\",\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        input_ids.append(encoded[\"input_ids\"])\n        attention_masks.append(encoded[\"attention_mask\"])\n\n    input_ids = torch.cat(input_ids,dim=0)\n    attention_masks = torch.cat(attention_masks,dim=0)\n    sentiment = torch.tensor(np.array(data['sentiment']))\n    return input_ids, attention_masks, sentiment","metadata":{"id":"if7JysJdWQ3H","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.concat([uit_train_data, train_data_1, train_data_2])\nval_data = pd.concat([uit_val_data, val_data_1, val_data_2])\ntest_data = pd.concat([uit_test_data, test_data_1, test_data_2])\ntrain_data.shape, val_data.shape, test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='sentiment', data=train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks, train_encoded_labels = encoding(train_data, tokenizer, max_token_len=MAX_LEN)\nval_input_ids, val_attention_masks, val_encoded_labels = encoding(val_data, tokenizer, max_token_len=MAX_LEN)\n    \ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_encoded_labels)\nval_dataset = TensorDataset(val_input_ids, val_attention_masks, val_encoded_labels)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"id":"nB7XynnHTwNL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class PhoBertBase(nn.Module):\n    def __init__(self, n_classes, drop_out=0.1):\n        super(PhoBertBase, self).__init__()\n        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n        self.l1 = torch.nn.Linear(768, 256)\n        self.l2 = torch.nn.Linear(256, n_classes)\n        self.d1 = torch.nn.Dropout(0.1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.bert(input_ids, attention_mask=attention_mask)\n        output = output[1]\n        output = self.l1(output)\n        output = self.d1(output)\n        output = self.l2(output)\n        return output","metadata":{"id":"wZNdoVK1YHzX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PhoBertLarge(nn.Module):\n    def __init__(self, n_classes, drop_out=0.1):\n        super(PhoBertLarge, self).__init__()\n        self.bert = AutoModel.from_pretrained(\"vinai/phobert-large\")\n        self.l1 = torch.nn.Linear(1024, 512)\n        self.l3 = torch.nn.Linear(512, n_classes)\n        self.d1 = torch.nn.Dropout(0.2)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.bert(input_ids, attention_mask=attention_mask)\n        output = output[1]\n        output = self.l1(output)\n        output = self.d1(output)\n        output = self.l3(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PhoBertBase(n_classes=3)\nmodel = model.to(device)\nmodel","metadata":{"id":"9qsDCpdhfu-S","outputId":"01c6260c-59c0-4fa2-dfc3-6f35a3e5c730","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, optimizer, scheduler, train_loader):\n    model.train()\n    total_loss = total = 0\n    total_correct = 0\n    progress_bar = tqdm(train_loader, desc='Training', leave=False)\n    predictions = []\n    labels = []\n    for batch in progress_bar:\n\n        label = batch[2].type(torch.LongTensor)\n        label = label.to(device)\n        input_ids = batch[0].to(device)\n        attention_masks = batch[1].to(device)\n\n        # Clean old gradients\n        optimizer.zero_grad()\n\n        # Forwards pass\n        output = model(input_ids, attention_masks)\n        \n        # Calculate how wrong the model is\n        loss = criterion(output, label)\n        preds = torch.argmax(output, dim=1)\n\n        total_correct += torch.sum(preds == label.data)\n        \n        # Perform gradient descent, backwards pass\n        loss.backward()\n\n        # Take a step in the right direction\n        optimizer.step()\n        scheduler.step()\n\n        # Record metrics\n        total_loss += loss.item()\n        total += len(label)\n\n    return total_correct / total, total_loss / total\n\n\ndef validate_epoch(model, valid_loader):\n    model.eval()\n    total_loss = total = 0\n    total_correct = 0\n    predictions = []\n    labels = []\n    with torch.no_grad():\n        progress_bar = tqdm(valid_loader, desc='Validating', leave=False)\n        for batch in progress_bar:\n            label = batch[2].type(torch.LongTensor)\n            label = label.to(device)\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n\n            # Forwards pass\n            output = model(input_ids, attention_masks)\n\n            preds = torch.argmax(output, 1)\n            predictions.append(preds.flatten())\n            labels.append(label.data)\n            \n            total_correct += torch.sum(preds == label.data)\n\n            # Calculate how wrong the model is\n            loss = criterion(output, label)\n\n            # Record metrics\n            total_loss += loss.item()\n            total += len(label)\n\n    predictions = torch.cat(predictions).detach().cpu()\n    labels = torch.cat(labels).detach().cpu()\n    print(classification_report(labels, predictions, digits = 4))\n    return total_correct / total, total_loss / total, predictions\n\n\ndef predict_test(model, test_loader):\n    model.eval()\n    predictions = []\n    labels = []\n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc='Validating', leave=False)\n        for batch in progress_bar:\n            label = batch[2].type(torch.LongTensor)\n            label = label.to(device)\n            input_ids = batch[0].to(device)\n            attention_masks = batch[1].to(device)\n\n            # Forwards pass\n            output = model(input_ids, attention_masks)\n\n            _, preds = torch.max(output, 1)\n            predictions.append(preds.flatten())\n            labels.append(label.data)\n\n    predictions = torch.cat(predictions).detach().cpu()\n    return predictions","metadata":{"id":"qWsbf1S2WfzA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = torch.FloatTensor([0.4, 0.25, 0.35])\ncriterion = nn.CrossEntropyLoss(weight=weights.to(device))\n# criterion = nn.CrossEntropyLoss()\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n]\n\noptimizer = optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=1e-8)\nnum_training_steps = EPOCHS * len(train_dataloader)\n\nlr_scheduler = get_scheduler(\n    'linear',\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)","metadata":{"id":"6G9KVbyrVfOl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_valid_loss = 1\ntrain_losses, valid_losses = [], []\n\nfor epoch in range(EPOCHS):\n\n    train_acc, train_loss = train_epoch(model, optimizer, lr_scheduler, train_dataloader)\n    valid_acc, valid_loss, val_pred = validate_epoch(model, val_dataloader)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), '/kaggle/working/best_model.pt')\n\n    print('Epoch {}/{}'.format(epoch, EPOCHS - 1))\n    print('-' * 10)\n    print('Training Loss: {:.2e} Acc: {:.8f}'.format(train_loss, train_acc))\n    print('Validate Loss: {:.2e} Acc: {:.8f}'.format(valid_loss, valid_acc))\n    # print('ROC AUC Score: {: .8f}'.format(roc_auc))\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    print()","metadata":{"id":"HvCzqIo5WitY","outputId":"5aec8178-c505-4a94-9f04-c16ab2000bd3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_ticks = range(1, EPOCHS + 1)\nplt.plot(epoch_ticks, train_losses)\nplt.plot(epoch_ticks, valid_losses)\nplt.legend(['Train Loss', 'Valid Loss'])\nplt.title('Losses') \nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.xticks(epoch_ticks)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PhoBertBase(n_classes=3)\nbest_model_cp = torch.load('/kaggle/working/best_model.pt')\nmodel.load_state_dict(best_model_cp)\nmodel = model.to(device)","metadata":{"id":"OvAJe7E6paqV","outputId":"b4519a07-8a6d-4736-ff40-0d00b31f797c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# uetcfs\ntest_input_ids_1, test_attention_masks_1, test_encoded_labels_1 = encoding(test_data_1, tokenizer, max_token_len=MAX_LEN)\ntest_dataset_1 = TensorDataset(test_input_ids_1, test_attention_masks_1, test_encoded_labels_1)\ntest_dataloader_1 = DataLoader(test_dataset_1, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# sguet\ntest_input_ids_2, test_attention_masks_2, test_encoded_labels_2 = encoding(test_data_2, tokenizer, max_token_len=MAX_LEN)\ntest_dataset_2 = TensorDataset(test_input_ids_2, test_attention_masks_2, test_encoded_labels_2)\ntest_dataloader_2 = DataLoader(test_dataset_2, batch_size=BATCH_SIZE, shuffle=False)\n\n# uit\ntest_input_ids_3, test_attention_masks_3, test_encoded_labels_3 = encoding(uit_test_data, tokenizer, max_token_len=MAX_LEN)\ntest_dataset_3 = TensorDataset(test_input_ids_3, test_attention_masks_3, test_encoded_labels_3)\ntest_dataloader_3 = DataLoader(test_dataset_3, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_uetcfs = predict_test(model, test_dataloader_1)\npredictions_sguet = predict_test(model, test_dataloader_2)\npredictions_uit = predict_test(model, test_dataloader_3)","metadata":{"id":"j4gGtItWpfpM","outputId":"8f8e2077-3395-494c-d3aa-6fbedf41e442","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"guess = pd.DataFrame()\nguess['content'] = test_data_1[\"content\"]\nguess['sentiment'] = list(map(float, predictions_uetcfs))\nguess","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(test_data_1['sentiment'], guess['sentiment'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_data_1['sentiment'], guess['sentiment'], digits = 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"guess = pd.DataFrame()\nguess['content'] = test_data_2[\"content\"]\nguess['sentiment'] = list(map(float, predictions_sguet))\nguess","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(test_data_2['sentiment'], guess['sentiment'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(test_data_2['sentiment'], guess['sentiment'], digits = 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"guess = pd.DataFrame()\nguess['content'] = uit_test_data[\"content\"]\nguess['sentiment'] = list(map(float, predictions_uit))\nguess","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(uit_test_data['sentiment'], guess['sentiment'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(uit_test_data['sentiment'], guess['sentiment'], digits = 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}